{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd4462b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8fdbe9",
   "metadata": {},
   "source": [
    "# Train BPETokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a01e13",
   "metadata": {},
   "source": [
    "## Train BPETokenizer on TinyStories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d73f582",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cs336_basics.tokenizer import BPETokenizer\n",
    "from cs336_basics.utils import ROOT_PATH\n",
    "\n",
    "tok = BPETokenizer()\n",
    "with open(ROOT_PATH / \"data/TinyStoriesV2-GPT4-train.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "tok.train(text, vocab_size=10000, verbose=True)\n",
    "tok.save(ROOT_PATH / \"tokenizer/tinystories_train_10000.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e06c64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check compression ratio.\n",
    "from cs336_basics.utils import compression_ratio\n",
    "from cs336_basics.tokenizer import BPETokenizer\n",
    "from cs336_basics.utils import ROOT_PATH\n",
    "\n",
    "tok = BPETokenizer.load(ROOT_PATH / \"tokenizer/tinystories_train_10000.pt\")\n",
    "\n",
    "with open(ROOT_PATH / \"data/TinyStoriesV2-GPT4-valid.txt\", \"r\") as f:\n",
    "    text = f.read(1_000_000) # Read only 1M characters.\n",
    "    print(f\"Compression ratio on TinyStories: {compression_ratio(tok, text):.2f}\")\n",
    "    # Compression ratio on TinyStories: 4.11\n",
    "\n",
    "with open(ROOT_PATH / \"data/owt_valid.txt\", \"r\") as f:\n",
    "    text = f.read(1_000_000) # Read only 1M characters.\n",
    "    print(f\"Compression ratio on OpenWebText: {compression_ratio(tok, text):.2f}\")\n",
    "    # Compression ratio on OpenWebText: 3.22"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06848fd5",
   "metadata": {},
   "source": [
    "## Train BPETokenizer on OpenWebText."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b413727e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cs336_basics.tokenizer import BPETokenizer\n",
    "from cs336_basics.utils import read_file_to_str_iterable\n",
    "from cs336_basics.utils import ROOT_PATH\n",
    "\n",
    "tok = BPETokenizer()\n",
    "# Use a generator instead of reading the whole file into memory.\n",
    "texts = read_file_to_str_iterable(ROOT_PATH / \"data/owt_train.txt\", special_tokens=[\"<|endoftext|>\"], buffer_size_bytes=100_000_000)\n",
    "tok.train_iterable(texts, vocab_size=32000, verbose=True)\n",
    "tok.save(ROOT_PATH / \"tokenizer/owt_train_32000.pt\")\n",
    "# Pretokenizing: 119it [26:27, 13.34s/it]\n",
    "# Computing pair counts: 6601892it [00:07, 857637.28it/s]\n",
    "# Merging: 100%|██████████| 31743/31743 [42:27<00:00, 12.46it/s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d663648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check compression rate on train and validation data.\n",
    "from cs336_basics.utils import compression_ratio\n",
    "from cs336_basics.tokenizer import BPETokenizer\n",
    "from cs336_basics.utils import ROOT_PATH\n",
    "\n",
    "tok = BPETokenizer.load(ROOT_PATH / \"tokenizer/owt_train_32000.pt\")\n",
    "\n",
    "with open(ROOT_PATH / \"data/TinyStoriesV2-GPT4-valid.txt\", \"r\") as f:\n",
    "    text = f.read(1_000_000) # Read only 1M characters.\n",
    "    print(f\"Compression ratio on TinyStories: {compression_ratio(tok, text):.2f}\")\n",
    "    # Compression ratio on TinyStories: 4.00\n",
    "\n",
    "with open(ROOT_PATH / \"data/owt_valid.txt\", \"r\") as f:\n",
    "    text = f.read(1_000_000) # Read only 1M characters.\n",
    "    print(f\"Compression ratio on OpenWebText: {compression_ratio(tok, text):.2f}\")\n",
    "    # Compression ratio on OpenWebText: 4.48"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223b5d45",
   "metadata": {},
   "source": [
    "# Encode train and validation data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deaa6b61",
   "metadata": {},
   "source": [
    "## Encode TinyStories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc69dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from cs336_basics.utils import ROOT_PATH, read_file_to_str_iterable\n",
    "from cs336_basics.tokenizer import BPETokenizer\n",
    "\n",
    "tok = BPETokenizer.load(ROOT_PATH / \"tokenizer/tinystories_train_10000.pt\")\n",
    "\n",
    "texts = read_file_to_str_iterable(\n",
    "    ROOT_PATH / \"data/TinyStoriesV2-GPT4-train.txt\",\n",
    "    special_tokens=[\"<|endoftext|>\"],\n",
    "    buffer_size_bytes=10_000_000,  # Read and encode ~10MB per time.\n",
    ")\n",
    "tokens = tok.encode_iterable(texts, verbose=True)\n",
    "np.save(ROOT_PATH / \"data/TinyStoriesV2-GPT4-train-tokens.npy\", np.array(list(tokens), dtype=np.uint16))\n",
    "\n",
    "texts = read_file_to_str_iterable(\n",
    "    ROOT_PATH / \"data/TinyStoriesV2-GPT4-valid.txt\",\n",
    "    special_tokens=[\"<|endoftext|>\"],\n",
    "    buffer_size_bytes=10_000_000,  # Read and encode ~10MB per time.\n",
    ")\n",
    "tokens = tok.encode_iterable(texts, verbose=True)\n",
    "np.save(ROOT_PATH / \"data/TinyStoriesV2-GPT4-valid-tokens.npy\", np.array(list(tokens), dtype=np.uint16))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee6a200",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cs336_basics.tokenizer import BPETokenizer\n",
    "\n",
    "tok = BPETokenizer.load(ROOT_PATH / \"tokenizer/tinystories_train_10000.pt\")\n",
    "\n",
    "tokens_memmap = np.load(ROOT_PATH / \"data/TinyStoriesV2-GPT4-valid-tokens.npy\", mmap_mode=\"r\")\n",
    "\n",
    "# Decode few pieces to visually check correctness.\n",
    "print(tok.decode(tokens_memmap[0:100]))\n",
    "print(\"------------------------------------------------------\")\n",
    "print(tok.decode(tokens_memmap[len(tokens_memmap)//2:len(tokens_memmap)//2+100]))\n",
    "print(\"------------------------------------------------------\")\n",
    "print(tok.decode(tokens_memmap[len(tokens_memmap)-100:len(tokens_memmap)]))\n",
    "\n",
    "# Check that decoding the tokens obtains the original text.\n",
    "print(\"------------------------------------------------------\")\n",
    "with open(ROOT_PATH / \"data/TinyStoriesV2-GPT4-valid.txt\", \"r\") as f:\n",
    "    text_orig = f.read()\n",
    "text_dec = tok.decode(tokens_memmap)\n",
    "print(\"Decoding works:\", text_orig == text_dec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33cb0cb4",
   "metadata": {},
   "source": [
    "## Encode OpenWebText\n",
    "\n",
    "Experiment here with flushing to memory the tokens every X MB. This allows\n",
    "keeping RAM consumption low at the cost of complicating a bit saving and loading\n",
    "of the numpy array as we have to write a byte file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f350ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cs336_basics.tokenizer import BPETokenizer\n",
    "import numpy as np\n",
    "from cs336_basics.utils import ROOT_PATH, read_file_to_str_iterable, write_int_iterable_to_byte_file\n",
    "\n",
    "tok = BPETokenizer.load(ROOT_PATH / \"tokenizer/owt_train_32000.pt\")\n",
    "\n",
    "texts = read_file_to_str_iterable(\n",
    "    ROOT_PATH / \"data/owt_train.txt\",\n",
    "    special_tokens=[\"<|endoftext|>\"],\n",
    "    buffer_size_bytes=10_000_000,  # Read and encode ~10MB per time.\n",
    ")\n",
    "tokens = tok.encode_iterable(texts, verbose=True)\n",
    "write_int_iterable_to_byte_file(\n",
    "    ROOT_PATH / \"data/owt_train_tokens.bin\",\n",
    "    ROOT_PATH / \"data/owt_train_tokens_metadata.json\",\n",
    "    tokens,\n",
    "    dtype=np.uint16,\n",
    "    buffer_size_bytes=100_000_000,  # Flush to memory every ~100MB.\n",
    ")\n",
    "# Encoding: 1264it [3:31:56, 10.06s/it]\n",
    "\n",
    "texts = read_file_to_str_iterable(\n",
    "    ROOT_PATH / \"data/owt_valid.txt\",\n",
    "    special_tokens=[\"<|endoftext|>\"],\n",
    "    buffer_size_bytes=10_000_000,  # Read and encode ~10MB per time.\n",
    ")\n",
    "tokens = tok.encode_iterable(texts, verbose=True)\n",
    "write_int_iterable_to_byte_file(\n",
    "    ROOT_PATH / \"data/owt_valid_tokens.bin\",\n",
    "    ROOT_PATH / \"data/owt_valid_tokens_metadata.json\",\n",
    "    tokens,\n",
    "    dtype=np.uint16,\n",
    "    buffer_size_bytes=100_000_000,  # Flush to memory every ~100MB.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb40aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cs336_basics.tokenizer import BPETokenizer\n",
    "from cs336_basics.utils import ROOT_PATH, read_byte_file_to_memmap\n",
    "\n",
    "tok = BPETokenizer.load(ROOT_PATH / \"tokenizer/owt_train_32000.pt\")\n",
    "\n",
    "tokens_memmap = read_byte_file_to_memmap(\n",
    "    ROOT_PATH / \"data/owt_valid_tokens.bin\",\n",
    "    ROOT_PATH / \"data/owt_valid_tokens_metadata.json\")\n",
    "\n",
    "# Decode few pieces to visually check correctness.\n",
    "print(tok.decode(tokens_memmap[0:100]))\n",
    "print(\"------------------------------------------------------\")\n",
    "print(tok.decode(tokens_memmap[len(tokens_memmap)//2:len(tokens_memmap)//2+100]))\n",
    "print(\"------------------------------------------------------\")\n",
    "print(tok.decode(tokens_memmap[len(tokens_memmap)-100:len(tokens_memmap)]))\n",
    "\n",
    "# Check that decoding the tokens obtains the original text.\n",
    "print(\"------------------------------------------------------\")\n",
    "with open(ROOT_PATH / \"data/owt_valid.txt\", \"r\") as f:\n",
    "    text_orig = f.read()\n",
    "text_dec = tok.decode(tokens_memmap)\n",
    "print(\"Decoding works:\", text_orig == text_dec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a437c17",
   "metadata": {},
   "source": [
    "# Train LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58f6cd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7885e1eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs336-basics (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
